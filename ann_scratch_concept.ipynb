{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ann_scratch_concept.ipynb","provenance":[],"collapsed_sections":["jsfX9qfdz5HS","8WSpqZp_0G7o","yV6HUN8D00J1"],"mount_file_id":"1xreovoPA1BcHIRoIbpf8LgB-uwEnYAvu","authorship_tag":"ABX9TyMWcni1vD5OkverK4o6reJI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# ANN from Scratch"],"metadata":{"id":"tOsJLu_DyF0g"}},{"cell_type":"markdown","source":["#### **Math in this notebook is not clear, avoid to use**"],"metadata":{"id":"BoWt9WIPV19P"}},{"cell_type":"markdown","source":["#### refer to: https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9"],"metadata":{"id":"dgCqCBtRyKtO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SdlCstf5xIjh"},"outputs":[],"source":["import json, numpy as np\n","np.random.seed(59)"]},{"cell_type":"markdown","source":["## define layer template"],"metadata":{"id":"zJyViEq1yvoa"}},{"cell_type":"markdown","source":["# #### *forward, calc network layer by layer; backward, error from layer output, adjust by derivative, return error to layer input*"],"metadata":{"id":"ad51bbBTzW4y"}},{"cell_type":"code","source":["class Layer:\n","    def __init__(self): pass\n","    def savelayer(self): return None\n","    def loadlayer(self, laydict): pass\n","\n","    def forward(self, input): return input\n","    def backward(self, input, err_out):\n","        num_units = input.shape[1]\n","        derivative = np.eye(num_units)\n","        return np.dot(err_out, derivative) # err_in,"],"metadata":{"id":"xchiBgbcyuO8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Activation layers, Sigmoid and ReLU"],"metadata":{"id":"jsfX9qfdz5HS"}},{"cell_type":"markdown","source":["#### Sigmoid function,  $ g(x) = \\frac{1}{1+e^{-x}}, g' = \\frac{e^{-x}}{(1+e^{-x})^2}= g(1-g)$, Sigmoid maps $(-\\infty, +\\infty)$ to (0 ,1), reverse function logit below"],"metadata":{"id":"8WSpqZp_0G7o"}},{"cell_type":"code","source":["class sigmoid(Layer):\n","    def __init__(self): pass\n","\n","    def forward(self, input):\n","        self.output = 1 / (1 + np.exp(-input))\n","        return self.output\n","    def backward(self, input, err_out):\n","        sigmoid_deriv = self.output * (1-self.output)\n","        return err_out * sigmoid_deriv\n"],"metadata":{"id":"n9-tFEeo1UQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ReLU function,  R(x) = max(0, x), R' = (x>0), 1 if x>0 else 0, in python T=1, F=0"],"metadata":{"id":"yV6HUN8D00J1"}},{"cell_type":"code","source":["class ReLU(Layer):\n","    def __init__(self): pass\n","    \n","    def forward(self, input):\n","        relu_out = np.maximum(0,input)\n","        return relu_out\n","    def backward(self, input, err_out):\n","        relu_grad = input > 0 # slope 1 or 0\n","        return err_out*relu_grad"],"metadata":{"id":"WaUu1rGG1WBD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dense layer"],"metadata":{"id":"9BS0cXgp5A6u"}},{"cell_type":"code","source":["class dense(Layer):\n","    def __init__(self, input_units, output_units, learning_rate=0.5):\n","        self.learning_rate = learning_rate\n","        self.wgs = np.random.normal(loc=0.0, scale = np.sqrt(2/(input_units+output_units)), size = (input_units,output_units))\n","        # Loc='mean of distribution', scale='standard deviation of the normal distribution', size='size and shape of the output'\n","        self.bias = np.zeros(output_units)\n","\n","    def savelayer(self): # save vertical stack wgs + bias\n","        return np.concatenate((self.wgs, [self.bias]), axis=0)\n","    \n","    def loadlayer(self, layerlist): # \n","        self.wgs = np.array(layerlist[:-1])\n","        self.bias = np.array(layerlist[-1])\n","    \n","    def forward(self, inputs):\n","        return np.dot(inputs, self.wgs) + self.bias\n","    \n","    def backward(self, inputs, err_out):\n","        err_in = np.dot(err_out, self.wgs.T)\n","        \n","        # compute gradient w.r.t. weights and biases\n","        delta_wgs = np.dot(inputs.T, err_out)\n","        delta_bias = err_out.mean(axis=0)*inputs.shape[0]\n","        \n","        assert delta_wgs.shape == self.wgs.shape and delta_bias.shape == self.bias.shape\n","        \n","        # Here we perform a stochastic gradient descent update step. \n","        self.wgs = self.wgs + self.learning_rate * delta_wgs\n","        self.bias = self.bias + self.learning_rate * delta_bias\n","        \n","        return err_in"],"metadata":{"id":"BK4svdhC5JLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cost function -- Softmax_crossEntropy and its gradient"],"metadata":{"id":"FFcmoJM97SCJ"}},{"cell_type":"markdown","source":["#### $logit = ln\\frac {p}{1-p}$, where: p=probability, $odds = \\frac{p}{1-p}$\n","logit function is reverse function of sigmoid, map (0, 1) domain to $(-\\infty, +\\infty)$"],"metadata":{"id":"B_dvk8vVQBjS"}},{"cell_type":"markdown","source":["#### cross entropy, loss = $a_{correct}-ln \\sum_{i}e^{a_i}$"],"metadata":{"id":"R4B6j7yrTfFh"}},{"cell_type":"code","source":["def softmax_crossentropy(logits, reference_answers):\n","    # Compute crossentropy from logits[batch, n_classes] and ids of correct answers\n","    logits_for_answers = logits[np.arange(len(logits)), reference_answers]\n","    \n","    xentropy = logits_for_answers - np.log(np.sum(np.exp(logits), axis=-1))\n","    return xentropy"],"metadata":{"id":"iTywMXnL7fyG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grad_softmax_crossentropy(logits, reference_answers):\n","    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n","    ones_for_answers = np.zeros_like(logits)\n","    ones_for_answers[np.arange(len(logits)), reference_answers] = 1\n","    \n","    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n","    return (ones_for_answers - softmax) / logits.shape[0]\n"],"metadata":{"id":"kgNTgmPi7jsw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Network"],"metadata":{"id":"cTX92rnoF_O1"}},{"cell_type":"code","source":["class yann: # need time to re-digest this block of code\n","    def __init__(self):\n","        pass\n","    \n","    def train_network(network, nnInputs, nnOutputs, epoches=1000, batch_size=0):\n","        if batch_size < 2:\n","            batch_size = len(nnInputs)\n","        for epoch in range(epoches): # epoch: times scan through whole data \n","            for kb in range(int(len(nnInputs)/batch_size)):\n","                if (kb+1)*batch_size>len(nnInputs):\n","                    layerIn = nnInputs[kb*batch_size:,:]\n","                    y_bat = nnOutputs[kb*batch_size:,:]\n","                else:\n","                    layerIn = nnInputs[kb*batch_size:(kb+1)*batch_size,:]\n","                    y_bat = nnOutputs[kb*batch_size:(kb+1)*batch_size,:]\n","                \n","                layerOuts = []; layerIns = []\n","                for layer in network:\n","                    layerIns.append(layerIn)\n","                    layerOuts.append(layer.forward(layerIn))\n","                    layerIn = layerOuts[-1]\n","                \n","                err_out = y_bat - layerOuts[-1]\n","                for layer_ix in range(len(network))[::-1]:\n","                    layer = network[layer_ix]\n","                    err_out = layer.backward(layerIns[layer_ix],err_out)   \n","            \n","    def evaluate_network(network, nnInputs):\n","        layerIn = nnInputs; layerOuts = []\n","        for layer in network:\n","            layerOuts.append(layer.forward(layerIn))\n","            layerIn = layerOuts[-1]\n","        print('training result: ')\n","        print(str(layerOuts[-1].T))\n","    \n","    def save_network(network, fname):\n","        network_data = []\n","        for layer in network:\n","            network_data.append(layer.savelayer())\n","        np.savetxt(fname, network_data, fmt='%s')\n","\n","    def load_network(network, fname):\n","        with open(fname, 'r') as fp:\n","            nnstr = \" \".join(fp.read().replace('\\n', ' ').replace('\\r', '')\n","             .split()).replace(' ', ',').replace('[,', '[').replace(',]', ']')\\\n","            .replace(']],', ']];').replace(',[[', ';[[')\n","            # read whole file as one string\n","        slayer = nnstr.split(';')\n","        # slayer = re.findall(\"(\\[{2}[^a-df-zA-DF-Z]+\\]{2})\", nnstr)\n","        for nl in range(len(slayer)):\n","            if slayer[nl] == 'None':\n","                continue\n","            network[nl].loadlayer(json.loads(slayer[nl]))\n","            # network[nl*2].loadlayer(json.loads(slayer[nl]))\n","        return network"],"metadata":{"id":"LYU9dvyHGEWr"},"execution_count":null,"outputs":[]}]}